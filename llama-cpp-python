sudo apt update
sudo apt install python3 python3-venv build-essential cmake -y

mkdir ~/ai-venv && cd ~/ai-venv
python3 -m venv venv
source venv/bin/activate

pip install --upgrade pip
pip install llama-cpp-python

mkdir models && cd models
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-GGUF/resolve/main/tinyllama-1.1b-chat.q4_K_M.gguf
cd ..

### CLI ###
[chat.py]
from llama_cpp import Llama

llm = Llama(
    model_path="./models/tinyllama-1.1b-chat.q4_K_M.gguf",
    n_ctx=512,
    verbose=True
)

while True:
    prompt = input("\nYou: ")
    if prompt.lower() in ["exit", "quit"]:
        break
    response = llm(prompt, max_tokens=256, stop=["You:"], echo=False)
    print("AI:", response["choices"][0]["text"].strip())

### FastAPI ###
source ~/ai-venv/venv/bin/activate
pip install fastapi uvicorn

[main.py]
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import FileResponse
from pydantic import BaseModel
from llama_cpp import Llama
import os

app = FastAPI()

llm = Llama(
    model_path="./models/tinyllama-1.1b-chat.q4_K_M.gguf",
    n_ctx=512,
    n_threads=4,
    n_batch=64,
    verbose=True,
)

class PromptRequest(BaseModel):
    prompt: str
    max_tokens: int = 256

@app.post("/chat")
def chat(request: PromptRequest):
    if not request.prompt.strip():
        raise HTTPException(status_code=400, detail="Empty prompt.")

    output = llm(
        request.prompt,
        max_tokens=request.max_tokens,
        stop=["User:", "You:"],
        echo=False
    )
    text = output["choices"][0]["text"].strip()
    return {"response": text}

@app.get("/")
def serve_index():
    return FileResponse("static/index.html")
[main.py] end

[static/index.html]
<!DOCTYPE html>
<html lang="bg">
<head>
  <meta charset="UTF-8">
  <title>–õ–æ–∫–∞–ª–µ–Ω AI —á–∞—Ç</title>
  <style>
    body {
      font-family: sans-serif;
      margin: 20px;
      max-width: 700px;
    }
    #response {
      white-space: pre-wrap;
      background: #f0f0f0;
      padding: 10px;
      margin-top: 10px;
      border-left: 3px solid #888;
    }
    input, button {
      font-size: 16px;
      padding: 10px;
      margin-top: 10px;
      width: 100%;
    }
  </style>
</head>
<body>
  <h2>üí¨ –õ–æ–∫–∞–ª–µ–Ω AI –ß–∞—Ç</h2>
  <textarea id="prompt" rows="4" placeholder="–ù–∞–ø–∏—à–∏ –Ω–µ—â–æ..."></textarea>
  <button onclick="sendPrompt()">–ò–∑–ø—Ä–∞—Ç–∏</button>
  <div id="response"></div>

  <script>
    async function sendPrompt() {
      const prompt = document.getElementById("prompt").value;
      const responseDiv = document.getElementById("response");
      responseDiv.innerText = "–ò–∑—á–∞–∫–≤–∞–Ω–µ –Ω–∞ –æ—Ç–≥–æ–≤–æ—Ä...";

      const res = await fetch("/chat", {
        method: "POST",
        headers: {"Content-Type": "application/json"},
        body: JSON.stringify({prompt: prompt})
      });

      if (!res.ok) {
        responseDiv.innerText = "–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–ø—Ä–∞—â–∞–Ω–µ.";
        return;
      }

      const data = await res.json();
      responseDiv.innerText = data.response;
    }
  </script>
</body>
</html>
[static/index.html] end

uvicorn main:app --host 0.0.0.0 --port 8000

curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is the capital of Bulgaria?"}'

[start-api.sh]
#!/bin/bash
cd ~/ai-venv
source venv/bin/activate
uvicorn main:app --host 127.0.0.1 --port 8000

[voice_chat.py]
import speech_recognition as sr
import pyttsx3
from llama_cpp import Llama

# –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ AI –º–æ–¥–µ–ª–∞
llm = Llama(
    model_path="./models/tinyllama-1.1b-chat.q4_K_M.gguf",
    n_ctx=512,
    n_threads=4,
    verbose=False
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ –≥–æ–≤–æ—Ä–µ—â–∏—è –≥–ª–∞—Å
engine = pyttsx3.init()
engine.setProperty("rate", 160)
engine.setProperty("voice", "bulgarian")  # –û–ø–∏—Ç–∞–π "bulgarian", "english"

# –†–∞–∑–ø–æ–∑–Ω–∞–≤–∞–Ω–µ –Ω–∞ —Ä–µ—á
recognizer = sr.Recognizer()
mic = sr.Microphone()

def speak(text):
    print(f"\nü§ñ AI: {text}")
    engine.say(text)
    engine.runAndWait()

def listen():
    with mic as source:
        print("\nüé§ –ö–∞–∂–∏ –Ω–µ—â–æ (–∏–ª–∏ '–∏–∑—Ö–æ–¥' –∑–∞ –∫—Ä–∞–π)...")
        recognizer.adjust_for_ambient_noise(source)
        audio = recognizer.listen(source)
    try:
        text = recognizer.recognize_google(audio, language="bg-BG")
        print(f"\nüó£Ô∏è –¢–∏ –∫–∞–∑–∞: {text}")
        return text
    except sr.UnknownValueError:
        print("ü§∑ –ù–µ —Ç–µ —Ä–∞–∑–±—Ä–∞—Ö.")
        return None
    except sr.RequestError as e:
        print(f"‚ö†Ô∏è –ì—Ä–µ—à–∫–∞ –≤ —Ä–∞–∑–ø–æ–∑–Ω–∞–≤–∞–Ω–µ: {e}")
        return None

def ask_ai(prompt):
    res = llm(prompt, max_tokens=256, stop=["User:", "You:"], echo=False)
    return res["choices"][0]["text"].strip()

# –ì–ª–∞–≤–µ–Ω —Ü–∏–∫—ä–ª
while True:
    user_text = listen()
    if user_text is None:
        continue
    if user_text.lower() in ["–∏–∑—Ö–æ–¥", "–∫—Ä–∞–π", "—Å–ø—Ä–∏"]:
        speak("–î–æ–≤–∏–∂–¥–∞–Ω–µ!")
        break
    reply = ask_ai(user_text)
    speak(reply)
[voice_chat.py] end

source ~/ai-venv/venv/bin/activate
pip install speechrecognition pyttsx3 pyaudio
sudo apt install portaudio19-dev python3-pyaudio

[voice_chat_vosk.py]
import queue
import sounddevice as sd
import pyttsx3
import json
from vosk import Model, KaldiRecognizer
from llama_cpp import Llama

# –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞ –∑–∞ —Ä–∞–∑–ø–æ–∑–Ω–∞–≤–∞–Ω–µ
model = Model("vosk-models/bg")
recognizer = KaldiRecognizer(model, 16000)
q = queue.Queue()

# –ì–ª–∞—Å–æ–≤ –∏–∑—Ö–æ–¥
engine = pyttsx3.init()
engine.setProperty("rate", 160)
engine.setProperty("voice", "bulgarian")

# –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ AI –º–æ–¥–µ–ª
llm = Llama(
    model_path="./models/tinyllama-1.1b-chat.q4_K_M.gguf",
    n_ctx=512,
    n_threads=4,
    verbose=False
)

def speak(text):
    print(f"\nü§ñ AI: {text}")
    engine.say(text)
    engine.runAndWait()

def callback(indata, frames, time, status):
    if status:
        print(status)
    q.put(bytes(indata))

def listen():
    print("\nüé§ –ì–æ–≤–æ—Ä–∏... (–∏–ª–∏ –∫–∞–∂–∏ ‚Äû–∏–∑—Ö–æ–¥‚Äú –∑–∞ –∫—Ä–∞–π)")
    with sd.RawInputStream(samplerate=16000, blocksize=8000, dtype='int16',
                           channels=1, callback=callback):
        result = ""
        while True:
            data = q.get()
            if recognizer.AcceptWaveform(data):
                res = json.loads(recognizer.Result())
                result = res.get("text", "")
                break
    print(f"üó£Ô∏è –¢–∏ –∫–∞–∑–∞: {result}")
    return result.strip()

def ask_ai(prompt):
    res = llm(prompt, max_tokens=256, stop=["User:", "You:"], echo=False)
    return res["choices"][0]["text"].strip()

# –ì–ª–∞–≤–µ–Ω —Ü–∏–∫—ä–ª
while True:
    user_input = listen()
    if not user_input:
        print("ü§∑ –ù–µ —Ç–µ —Ä–∞–∑–±—Ä–∞—Ö.")
        continue
    if user_input.lower() in ["–∏–∑—Ö–æ–¥", "–∫—Ä–∞–π", "—Å–ø—Ä–∏"]:
        speak("–î–æ–≤–∏–∂–¥–∞–Ω–µ!")
        break
    reply = ask_ai(user_input)
    speak(reply)
[voice_chat_vosk.py] end

source ~/ai-venv/venv/bin/activate
pip install vosk sounddevice pyttsx3
mkdir -p ~/ai-venv/vosk-models && cd ~/ai-venv/vosk-models
wget https://alphacephei.com/vosk/models/vosk-model-small-bg-0.4.zip
unzip vosk-model-small-bg-0.4.zip
mv vosk-model-small-bg-0.4 bg

arecord -l
sd.default.device = 1  # –∏–ª–∏ ID –æ—Ç arecord, –∏–∑–±–æ—Ä –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ

