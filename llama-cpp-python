sudo apt update
sudo apt install python3 python3-venv build-essential cmake -y

mkdir ~/ai-venv && cd ~/ai-venv
python3 -m venv venv
source venv/bin/activate

pip install --upgrade pip
pip install llama-cpp-python

mkdir models && cd models
wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-GGUF/resolve/main/tinyllama-1.1b-chat.q4_K_M.gguf
cd ..

### CLI ###
[chat.py]
from llama_cpp import Llama

llm = Llama(
    model_path="./models/tinyllama-1.1b-chat.q4_K_M.gguf",
    n_ctx=512,
    verbose=True
)

while True:
    prompt = input("\nYou: ")
    if prompt.lower() in ["exit", "quit"]:
        break
    response = llm(prompt, max_tokens=256, stop=["You:"], echo=False)
    print("AI:", response["choices"][0]["text"].strip())

### FastAPI ###
source ~/ai-venv/venv/bin/activate
pip install fastapi uvicorn

[main.py]
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import FileResponse
from pydantic import BaseModel
from llama_cpp import Llama
import os

app = FastAPI()

llm = Llama(
    model_path="./models/tinyllama-1.1b-chat.q4_K_M.gguf",
    n_ctx=512,
    n_threads=4,
    n_batch=64,
    verbose=True,
)

class PromptRequest(BaseModel):
    prompt: str
    max_tokens: int = 256

@app.post("/chat")
def chat(request: PromptRequest):
    if not request.prompt.strip():
        raise HTTPException(status_code=400, detail="Empty prompt.")

    output = llm(
        request.prompt,
        max_tokens=request.max_tokens,
        stop=["User:", "You:"],
        echo=False
    )
    text = output["choices"][0]["text"].strip()
    return {"response": text}

@app.get("/")
def serve_index():
    return FileResponse("static/index.html")
[main.py] end

[static/index.html]
<!DOCTYPE html>
<html lang="bg">
<head>
  <meta charset="UTF-8">
  <title>Ð›Ð¾ÐºÐ°Ð»ÐµÐ½ AI Ñ‡Ð°Ñ‚</title>
  <style>
    body {
      font-family: sans-serif;
      margin: 20px;
      max-width: 700px;
    }
    #response {
      white-space: pre-wrap;
      background: #f0f0f0;
      padding: 10px;
      margin-top: 10px;
      border-left: 3px solid #888;
    }
    input, button {
      font-size: 16px;
      padding: 10px;
      margin-top: 10px;
      width: 100%;
    }
  </style>
</head>
<body>
  <h2>ðŸ’¬ Ð›Ð¾ÐºÐ°Ð»ÐµÐ½ AI Ð§Ð°Ñ‚</h2>
  <textarea id="prompt" rows="4" placeholder="ÐÐ°Ð¿Ð¸ÑˆÐ¸ Ð½ÐµÑ‰Ð¾..."></textarea>
  <button onclick="sendPrompt()">Ð˜Ð·Ð¿Ñ€Ð°Ñ‚Ð¸</button>
  <div id="response"></div>

  <script>
    async function sendPrompt() {
      const prompt = document.getElementById("prompt").value;
      const responseDiv = document.getElementById("response");
      responseDiv.innerText = "Ð˜Ð·Ñ‡Ð°ÐºÐ²Ð°Ð½Ðµ Ð½Ð° Ð¾Ñ‚Ð³Ð¾Ð²Ð¾Ñ€...";

      const res = await fetch("/chat", {
        method: "POST",
        headers: {"Content-Type": "application/json"},
        body: JSON.stringify({prompt: prompt})
      });

      if (!res.ok) {
        responseDiv.innerText = "Ð“Ñ€ÐµÑˆÐºÐ° Ð¿Ñ€Ð¸ Ð¸Ð·Ð¿Ñ€Ð°Ñ‰Ð°Ð½Ðµ.";
        return;
      }

      const data = await res.json();
      responseDiv.innerText = data.response;
    }
  </script>
</body>
</html>
[static/index.html] end

uvicorn main:app --host 0.0.0.0 --port 8000

curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What is the capital of Bulgaria?"}'

[start-api.sh]
#!/bin/bash
cd ~/ai-venv
source venv/bin/activate
uvicorn main:app --host 127.0.0.1 --port 8000
