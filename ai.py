### Na√Øve Bayes Classifier ###

from sklearn.feature_extraction.text import CountVectorizer                                                                        
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline

# –ü—Ä–∏–º–µ—Ä–Ω–∏ –¥–∞–Ω–Ω–∏: –¥–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—Å–ø–∞–º –∏ –Ω–æ—Ä–º–∞–ª–Ω–∏ —Å—ä–æ–±—â–µ–Ω–∏—è)
X_train = ["–ü—Ä–æ–º–æ—Ü–∏—è! –°–ø–µ—á–µ–ª–∏ –Ω–∞–≥—Ä–∞–¥–∞ —Å–µ–≥–∞!", 
           "–ó–¥—Ä–∞–≤–µ–π, –∫–∞–∫ —Å–∏ –¥–Ω–µ—Å?", 
           "–ö—É–ø–∏ —Å–µ–≥–∞ –∏ –ø–æ–ª—É—á–∏ 50% –æ—Ç—Å—Ç—ä–ø–∫–∞!", 
           "–ì—Ä–∞–Ω–¥–∏–æ–∑–Ω–∞ –æ—Ñ–µ—Ä—Ç–∞! –ë–µ–∑–ø–ª–∞—Ç–µ–Ω iPhone!", 
           "–¢–æ–≤–∞ –µ –æ–±–∏–∫–Ω–æ–≤–µ–Ω –∏–º–µ–π–ª –±–µ–∑ —Ä–µ–∫–ª–∞–º–∞."]
y_train = ["spam", "normal", "spam", "spam", "normal"]

# –°—ä–∑–¥–∞–≤–∞–º–µ –º–æ–¥–µ–ª
model = make_pipeline(CountVectorizer(), MultinomialNB())

# –¢—Ä–µ–Ω–∏—Ä–∞–º–µ –º–æ–¥–µ–ª–∞
model.fit(X_train, y_train)

# –¢–µ—Å—Ç–≤–∞–º–µ —Å –Ω–æ–≤ —Ç–µ–∫—Å—Ç
test_messages = ["–°–ø–µ—Ü–∏–∞–ª–Ω–∞ –æ—Ñ–µ—Ä—Ç–∞ –∑–∞ —Ç–µ–±!", "–ö–∞–∫ –º–∏–Ω–∞ —Å—Ä–µ—â–∞—Ç–∞ –¥–Ω–µ—Å?"]
predictions = model.predict(test_messages)

# –ò–∑–≤–µ–∂–¥–∞–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ
print(predictions)  # ['spam', 'normal']


### –ê–Ω–∞–ª–∏–∑ ###
## Bag of Words (–ß–∞–Ω—Ç–∞ —Å –¥—É–º–∏) ##
# –¢–æ–∑–∏ –º–µ—Ç–æ–¥ –ø–æ–¥–æ–±—Ä—è–≤–∞ Bag of Words, –∫–∞—Ç–æ –Ω–µ —Å–∞–º–æ –±—Ä–æ–∏ –¥—É–º–∏—Ç–µ, –Ω–æ –∏ –¥–∞–≤–∞ –ø–æ-–º–∞–ª–∫–∞ —Ç–µ–∂–µ—Å—Ç –Ω–∞ —á–µ—Å—Ç–æ —Å—Ä–µ—â–∞–Ω–∏—Ç–µ –¥—É–º–∏
# (–∫–∞—Ç–æ "–∏", "–∑–∞") –∏ –ø–æ-–≥–æ–ª—è–º–∞ —Ç–µ–∂–µ—Å—Ç –Ω–∞ –≤–∞–∂–Ω–∏—Ç–µ –¥—É–º–∏.
# –§–æ—Ä–º—É–ª–∞—Ç–∞ –∑–∞ TF-IDF –µ: ùëáùêπ ‚àí ùêºùê∑ùêπ = ùëáùêπ √ó ùêºùê∑ùêπ
# –∫—ä–¥–µ—Ç–æ: TF (Term Frequency) ‚Äì —á–µ—Å—Ç–æ—Ç–∞ –Ω–∞ –¥–∞–¥–µ–Ω–∞ –¥—É–º–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
#         IDF (Inverse Document Frequency) ‚Äì –∫–æ–ª–∫–æ "—Ä—è–¥–∫–∞" –µ –¥—É–º–∞—Ç–∞ –≤ —Ü–µ–ª–∏—è –∫–æ—Ä–ø—É—Å.

texts = X_train
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

print(vectorizer.get_feature_names_out())
print(X.toarray())


### –ê–Ω–∞–ª–∏–∑ ###
## TF-IDF (Term Frequency - Inverse Document Frequency) ##
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

print(vectorizer.get_feature_names_out())
print(X.toarray())  # –°—Ç–æ–π–Ω–æ—Å—Ç–∏—Ç–µ —â–µ –±—ä–¥–∞—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ –∏ –ø—Ä–µ—Ç–µ–≥–ª–µ–Ω–∏



### Word2Vec ###
# Word embeddings –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–≤–∞—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω–∏ –≤–µ–∫—Ç–æ—Ä–∏, –∫–æ–∏—Ç–æ —É–ª–∞–≤—è—Ç —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ –¥—É–º–∏—Ç–µ.
# –ï–¥–∏–Ω –æ—Ç –Ω–∞–π-–ø–æ–ø—É–ª—è—Ä–Ω–∏—Ç–µ –º–µ—Ç–æ–¥–∏ –∑–∞ —Å—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ —Ç–∞–∫–∏–≤–∞ –≤–µ–∫—Ç–æ—Ä–∏ –µ Word2Vec, –∫–æ–π—Ç–æ –∏–∑–ø–æ–ª–∑–≤–∞ –Ω–µ–≤—Ä–æ–Ω–Ω–∏ –º—Ä–µ–∂–∏,
# –∑–∞ –¥–∞ –Ω–∞—É—á–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–∏—è –Ω–∞ –¥—É–º–∏ –Ω–∞ –±–∞–∑–∞—Ç–∞ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –≤ –∫–æ–π—Ç–æ —Å–µ —Å—Ä–µ—â–∞—Ç.

from gensim.models import Word2Vec                                                                                                 

# –ü—Ä–∏–º–µ—Ä–Ω–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è
sentences = [
    ["–∫—É—á–µ", "–ª–∞–µ", "–Ω–∞", "–∫–æ—Ç–∫–∞"],
    ["–∫–æ—Ç–∫–∞", "—Å–∫–∞—á–∞", "–Ω–∞", "–º–∞—Å–∞—Ç–∞"],
    ["–ø—Ç–∏—Ü–∞", "–ª–µ—Ç–∏", "–Ω–∞–¥", "–¥—ä—Ä–≤–æ—Ç–æ"],
    ["–∫—É—á–µ", "–≥–æ–Ω–∏", "—Ç–æ–ø–∫–∞"],
    ["–∫–æ—Ç–∫–∞", "–ª–µ–∂–∏", "–Ω–∞", "–¥–∏–≤–∞–Ω–∞"],
    ["–ø—Ç–∏—Ü–∞", "–ø–µ–µ", "–Ω–∞", "–∫–ª–æ–Ω–∞"]
]

# –û–±—É—á–∞–≤–∞–º–µ Word2Vec –º–æ–¥–µ–ª–∞
# vector_size=10 ‚Äì —Ä–∞–∑–º–µ—Ä—ä—Ç –Ω–∞ –≤–µ–∫—Ç–æ—Ä–∏—Ç–µ (–º–æ–∂–µ –¥–∞ —Å–µ —É–≤–µ–ª–∏—á–∏ –∑–∞ –ø–æ-–¥–æ–±—Ä–æ –∫–∞—á–µ—Å—Ç–≤–æ).
# window=2 ‚Äì –±—Ä–æ–π —Å—ä—Å–µ–¥–Ω–∏ –¥—É–º–∏, –∫–æ–∏—Ç–æ –º–æ–¥–µ–ª—ä—Ç —â–µ –∏–∑–ø–æ–ª–∑–≤–∞ –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
# min_count=1 ‚Äì –º–∏–Ω–∏–º–∞–ª–µ–Ω –±—Ä–æ–π —Å—Ä–µ—â–∞–Ω–∏—è –Ω–∞ –¥—É–º–∞ –≤ –æ–±—É—á–∞–≤–∞—â–∏—Ç–µ –¥–∞–Ω–Ω–∏.
# workers=4 ‚Äì –∏–∑–ø–æ–ª–∑–≤–∞ 4 CPU –Ω–∏—à–∫–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ.
model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, workers=4)

# –ò–∑–≤–ª–∏—á–∞–º–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤—è–Ω–µ –Ω–∞ –¥—É–º–∞—Ç–∞ "–∫—É—á–µ"
vector = model.wv["–∫—É—á–µ"]
print("–í–µ–∫—Ç–æ—Ä –∑–∞ '–∫—É—á–µ':", vector)

# –ù–∞–º–∏—Ä–∞–º–µ –Ω–∞–π-–±–ª–∏–∑–∫–∏—Ç–µ –¥—É–º–∏ –¥–æ "–∫—É—á–µ"
similar_words = model.wv.most_similar("–∫—É—á–µ", topn=3)
print("–ù–∞–π-–±–ª–∏–∑–∫–∏ –¥—É–º–∏ –¥–æ '–∫—É—á–µ':", similar_words)


### Word2Vec + Prompt Matching ###
from gensim.models import Word2Vec
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
# –ü—Ä–∏–º–µ—Ä–Ω–∏ –∏–∑—Ä–µ—á–µ–Ω–∏—è (–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏—è)
knowledge_base = [
    "–ö–∞–∫–≤–æ –µ –º–∞—à–∏–Ω–Ω–æ –æ–±—É—á–µ–Ω–∏–µ?",
    "–ö–∞–∫ —Ä–∞–±–æ—Ç–∏ Word2Vec?",
    "–ö–∞–∫–≤–æ –µ –Ω–µ–≤—Ä–æ–Ω–Ω–∞ –º—Ä–µ–∂–∞?",
    "–ö–∞–∫ –¥–∞ –∏–∑–ø–æ–ª–∑–≤–∞–º Python –∑–∞ NLP?",
    "–ö–∞–∫ —Ä–∞–±–æ—Ç–∏ Na√Øve Bayes –∫–ª–∞—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ä—Ç?"
]

# –û–±—É—á–∞–≤–∞–º–µ Word2Vec –º–æ–¥–µ–ª–∞ –≤—ä—Ä—Ö—É –±–∞–∑–∞—Ç–∞ –∑–Ω–∞–Ω–∏—è
tokenized_sentences = [sentence.lower().split() for sentence in knowledge_base]
model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)

# –§—É–Ω–∫—Ü–∏—è –∑–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–≤–∞–Ω–µ –Ω–∞ –∏–∑—Ä–µ—á–µ–Ω–∏–µ –≤—ä–≤ –≤–µ–∫—Ç–æ—Ä (—É—Å—Ä–µ–¥–Ω–µ–Ω –≤–µ–∫—Ç–æ—Ä –Ω–∞ –¥—É–º–∏—Ç–µ)
def sentence_vector(sentence, model):
    words = sentence.lower().split()
    word_vectors = [model.wv[word] for word in words if word in model.wv]
    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)

# –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –≤–µ–∫—Ç–æ—Ä–∏ –∑–∞ –±–∞–∑–∞—Ç–∞ –∑–Ω–∞–Ω–∏—è
kb_vectors = [sentence_vector(sent, model) for sent in knowledge_base]

# –í—Ö–æ–¥–µ–Ω prompt –æ—Ç –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è
user_prompt = "–ö–∞–∫ —Ä–∞–±–æ—Ç–∏ NLP —Å Python?"
prompt_vector = sentence_vector(user_prompt, model)

# –ù–∞–º–∏—Ä–∞–º–µ –Ω–∞–π-–±–ª–∏–∑–∫–æ—Ç–æ –∏–∑—Ä–µ—á–µ–Ω–∏–µ —á—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–æ–≤–∞ –±–ª–∏–∑–æ—Å—Ç
similarities = cosine_similarity([prompt_vector], kb_vectors)
best_match_index = np.argmax(similarities)

# –ò–∑–≤–µ–∂–¥–∞–º–µ –Ω–∞–π-–ø–æ–¥—Ö–æ–¥—è—â–∏—è –æ—Ç–≥–æ–≤–æ—Ä
print("–í—ä–ø—Ä–æ—Å: ", user_prompt)                             
print("–ù–∞–π-–ø–æ–¥—Ö–æ–¥—è—â –æ—Ç–≥–æ–≤–æ—Ä:", knowledge_base[best_match_index])



import os

def get_files(directory, ext):
    python_files = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(ext):
                python_files.append(os.path.join(root, file))
    return python_files

def read_code(files):
    code_lines = []
    for file in files:
        with open(file, "r", encoding="utf-8") as f:
            code = f.readlines()
            tokenized_lines = [line.strip().split() for line in code if line.strip()]
            code_lines.extend(tokenized_lines)
    return code_lines

# –ü—Ä–∏–º–µ—Ä: —Å—ä–±–∏—Ä–∞–Ω–µ –Ω–∞ –∫–æ–¥ –æ—Ç "my_project/"
python_code_files = get_python_files("my_project/", ".py")
print("–ù–∞–º–µ—Ä–µ–Ω–∏ —Ñ–∞–π–ª–æ–≤–µ:", python_code_files)

# –ó–∞—Ä–µ–∂–¥–∞–º–µ –∏ –æ–±—Ä–∞–±–æ—Ç–≤–∞–º–µ –∫–æ–¥–∞
tokenized_code = read_python_code(python_code_files)

# –û–±—É—á–∞–≤–∞–º–µ Word2Vec –≤—ä—Ä—Ö—É –∫–æ–¥–∞
model = Word2Vec(tokenized_code, vector_size=100, window=5, min_count=1, workers=4)
model.save("word2vec_code.model")  # –ó–∞–ø–∞–∑–≤–∞–º–µ –º–æ–¥–µ–ª–∞ –∑–∞ –±—ä–¥–µ—â–∞ —É–ø–æ—Ç—Ä–µ–±–∞
print("–ú–æ–¥–µ–ª—ä—Ç –µ –æ–±—É—á–µ–Ω –∏ –∑–∞–ø–∏—Å–∞–Ω.")


## –û–±—Ä–∞–±–æ—Ç–≤–∞–Ω–µ –Ω–∞ Prompt –∏ –¢—ä—Ä—Å–µ–Ω–µ –Ω–∞ –ü–æ–¥—Ö–æ–¥—è—â –ö–æ–¥ ##

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# –ó–∞—Ä–µ–∂–¥–∞–º–µ –æ–±—É—á–µ–Ω –º–æ–¥–µ–ª
model = Word2Vec.load("word2vec_code.model")

# –§—É–Ω–∫—Ü–∏—è –∑–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–≤–∞–Ω–µ –Ω–∞ —Ç–µ–∫—Å—Ç –≤—ä–≤ –≤–µ–∫—Ç–æ—Ä
def sentence_vector(sentence, model):
    words = sentence.lower().split()
    word_vectors = [model.wv[word] for word in words if word in model.wv]
    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–≤–∞–º–µ –≤—Å–∏—á–∫–∏ —Ä–µ–¥–æ–≤–µ –æ—Ç –∫–æ–¥–∞ –≤ –≤–µ–∫—Ç–æ—Ä–∏
code_vectors = [sentence_vector(" ".join(line), model) for line in tokenized_code]

# –¢—ä—Ä—Å–µ–Ω–µ –ø–æ prompt
def find_best_code_match(prompt):
    prompt_vector = sentence_vector(prompt, model)
    similarities = cosine_similarity([prompt_vector], code_vectors)
    best_match_index = np.argmax(similarities)
    return " ".join(tokenized_code[best_match_index])

# –ü—Ä–∏–º–µ—Ä–µ–Ω prompt
user_prompt = "—á–µ—Ç–∏ JSON —Ñ–∞–π–ª"
best_match = find_best_code_match(user_prompt)
print("–ù–∞–π-–ø–æ–¥—Ö–æ–¥—è—â –∫–æ–¥:", best_match)

## –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –Ω–æ–≤ –∫–æ–¥ –≤—ä–∑ –æ—Å–Ω–æ–≤–∞ –Ω–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∞ ##
import openai  # –ê–∫–æ –∏–∑–ø–æ–ª–∑–≤–∞—à OpenAI API

openai.api_key = "API_KEY"

def generate_code(prompt, base_code):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "–¢–∏ —Å–∏ –∞—Å–∏—Å—Ç–µ–Ω—Ç –∑–∞ –ø–∏—Å–∞–Ω–µ –Ω–∞ Python –∫–æ–¥."},
            {"role": "user", "content": f"–î–æ–≤—ä—Ä—à–∏ —Ç–æ–∑–∏ –∫–æ–¥:\n{base_code}\n\n{prompt}"}
        ]
    )
    return response["choices"][0]["message"]["content"]

generated_code = generate_code(user_prompt, best_match)
print("–ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω –∫–æ–¥:\n", generated_code)

## –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –§—É–Ω–∫—Ü–∏–∏ –æ—Ç Python –ö–æ–¥–∞ ##
import os
import ast

def extract_functions_from_file(filepath):
    """ –ò–∑–≤–ª–∏—á–∞ –≤—Å–∏—á–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç –¥–∞–¥–µ–Ω Python —Ñ–∞–π–ª. """
    with open(filepath, "r", encoding="utf-8") as file:
        code = file.read()

    tree = ast.parse(code)
    functions = []

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):  # –ù–∞–º–µ—Ä–µ–Ω–∞ –µ —Ñ—É–Ω–∫—Ü–∏—è
            function_code = ast.unparse(node)  # –í—Ä—ä—â–∞ –∫–æ–¥–∞ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è—Ç–∞
            functions.append(function_code)

    return functions

def get_all_functions(directory):
    """ –ò–∑–≤–ª–∏—á–∞ –≤—Å–∏—á–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç –≤—Å–∏—á–∫–∏ Python —Ñ–∞–π–ª–æ–≤–µ –≤ –ø—Ä–æ–µ–∫—Ç–∞. """
    all_functions = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".py"):
                file_path = os.path.join(root, file)
                all_functions.extend(extract_functions_from_file(file_path))
    return all_functions

# –ü—Ä–∏–º–µ—Ä: –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –≤—Å–∏—á–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç –ø—Ä–æ–µ–∫—Ç–∞
project_path = "my_project/"
python_functions = get_all_functions(project_path)

print(f"–ù–∞–º–µ—Ä–µ–Ω–∏ {len(python_functions)} —Ñ—É–Ω–∫—Ü–∏–∏.")
print("–ü—Ä–∏–º–µ—Ä–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è:\n", python_functions[0])

## –¢—Ä–µ–Ω–∏—Ä–∞–Ω–µ –Ω–∞ Word2Vec –≤—ä—Ä—Ö—É —Ñ—É–Ω–∫—Ü–∏–∏—Ç–µ ##
from gensim.models import Word2Vec
import re

def tokenize_function_code(function_code):
    """ –†–∞–∑–¥–µ–ª—è —Ñ—É–Ω–∫—Ü–∏—è—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª–Ω–∏ –¥—É–º–∏ –∏ —Ç–æ–∫–µ–Ω–∏. """
    tokens = re.findall(r"\b\w+\b", function_code)  # –í–∑–∏–º–∞ –≤—Å–∏—á–∫–∏ –¥—É–º–∏ –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∏
    return tokens

# –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–∞–º–µ –≤—Å–∏—á–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏
tokenized_functions = [tokenize_function_code(func) for func in python_functions]

# –û–±—É—á–∞–≤–∞–º–µ Word2Vec –≤—ä—Ä—Ö—É —Ñ—É–Ω–∫—Ü–∏–∏—Ç–µ
model = Word2Vec(tokenized_functions, vector_size=100, window=5, min_count=1, workers=4)
model.save("word2vec_functions.model")  # –ó–∞–ø–∞–∑–≤–∞–º–µ –º–æ–¥–µ–ª–∞
print("–û–±—É—á–µ–Ω–∏–µ—Ç–æ –Ω–∞ Word2Vec –µ –∑–∞–≤—ä—Ä—à–µ–Ω–æ.")

## –¢—ä—Ä—Å–µ–Ω–µ –Ω–∞ –ù–∞–π-–ë–ª–∏–∑–∫–∞ –§—É–Ω–∫—Ü–∏—è –∫—ä–º Prompt ##
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# –ó–∞—Ä–µ–∂–¥–∞–º–µ –æ–±—É—á–µ–Ω –º–æ–¥–µ–ª
model = Word2Vec.load("word2vec_functions.model")

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–≤–∞–º–µ —Ü—è–ª–∞ —Ñ—É–Ω–∫—Ü–∏—è –≤—ä–≤ –≤–µ–∫—Ç–æ—Ä
def function_vector(function_code, model):
    tokens = tokenize_function_code(function_code)
    word_vectors = [model.wv[word] for word in tokens if word in model.wv]
    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.vector_size)

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–≤–∞–º–µ –≤—Å–∏—á–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –≤–µ–∫—Ç–æ—Ä–∏
function_vectors = [function_vector(func, model) for func in python_functions]

# –¢—ä—Ä—Å–µ–Ω–µ –Ω–∞ –Ω–∞–π-–±–ª–∏–∑–∫–∞ —Ñ—É–Ω–∫—Ü–∏—è
def find_best_function(prompt):
    prompt_vector = function_vector(prompt, model)
    similarities = cosine_similarity([prompt_vector], function_vectors)
    best_match_index = np.argmax(similarities)
    return python_functions[best_match_index]

# –ü—Ä–∏–º–µ—Ä–µ–Ω prompt
user_prompt = "–ø—Ä–æ—á–µ—Ç–∏ json —Ñ–∞–π–ª"
best_function = find_best_function(user_prompt)

print("–ù–∞–π-–±–ª–∏–∑–∫–∞—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è:\n", best_function)

## –ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –ù–æ–≤ –ö–æ–¥ ##
import openai

openai.api_key = "API_KEY"

def generate_code(prompt, base_function):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "–¢–∏ —Å–∏ –∞—Å–∏—Å—Ç–µ–Ω—Ç –∑–∞ –ø–∏—Å–∞–Ω–µ –Ω–∞ Python –∫–æ–¥."},
            {"role": "user", "content": f"–î–æ–ø—ä–ª–Ω–∏ –∏–ª–∏ –ø–æ–¥–æ–±—Ä–∏ —Ç–∞–∑–∏ —Ñ—É–Ω–∫—Ü–∏—è:\n{base_function}\n\n{prompt}"}
        ]
    )
    return response["choices"][0]["message"]["content"]

generated_code = generate_code(user_prompt, best_function)
print("–ì–µ–Ω–µ—Ä–∏—Ä–∞–Ω –∫–æ–¥:\n", generated_code)

## –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞ –≤–µ—á–µ –æ–±—É—á–µ–Ω –º–æ–¥–µ–ª ##
import os
from gensim.models import Word2Vec

MODEL_PATH = "word2vec_functions.model"

def load_existing_model():
    """ –ó–∞—Ä–µ–∂–¥–∞ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏—è Word2Vec –º–æ–¥–µ–ª, –∞–∫–æ –∏–º–∞ —Ç–∞–∫—ä–≤. """
    if os.path.exists(MODEL_PATH):
        print("üîÑ –ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏—è Word2Vec –º–æ–¥–µ–ª...")
        return Word2Vec.load(MODEL_PATH)
    else:
        print("üÜï –ù—è–º–∞ –Ω–∞–ª–∏—á–µ–Ω –º–æ–¥–µ–ª. –©–µ —Å—ä–∑–¥–∞–¥–µ–º –Ω–æ–≤.")
        return None

existing_model = load_existing_model()

## –ò–∑–≤–ª–∏—á–∞–Ω–µ –Ω–∞ –ù–æ–≤–∏ –§—É–Ω–∫—Ü–∏–∏ –∏ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞ –î–∞–Ω–Ω–∏—Ç–µ ##
def get_new_functions(existing_functions, project_path):
    """ –ù–∞–º–µ—Ä–∏ –Ω–æ–≤–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ–∏—Ç–æ –Ω–µ —Å–∞ –±–∏–ª–∏ –≤–∫–ª—é—á–µ–Ω–∏ –¥–æ—Å–µ–≥–∞. """
    all_functions = get_all_functions(project_path)  # –í—Å–∏—á–∫–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Ç –ø—Ä–æ–µ–∫—Ç–∞
    new_functions = [func for func in all_functions if func not in existing_functions]
    return new_functions

new_functions = get_new_functions(existing_model.wv.index_to_key if existing_model else [], "my_project/")

if new_functions:
    print(f"üîç –ù–∞–º–µ—Ä–µ–Ω–∏ {len(new_functions)} –Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ.")
else:
    print("‚úÖ –ù—è–º–∞ –Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏. –ú–æ–¥–µ–ª—ä—Ç –µ –∞–∫—Ç—É–∞–ª–µ–Ω.")

## –î–æ–æ–±—É—á–∞–≤–∞–Ω–µ –Ω–∞ Word2Vec —Å –Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏ ##
if new_functions and existing_model:
    print("üîÑ –î–æ–æ–±—É—á–∞–≤–∞–º–µ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏—è –º–æ–¥–µ–ª —Å –Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏...")
    tokenized_new_functions = [tokenize_function_code(func) for func in new_functions]
    
    # –î–æ–æ–±—É—á–∞–≤–∞–º–µ –º–æ–¥–µ–ª–∞
    existing_model.build_vocab(tokenized_new_functions, update=True)
    existing_model.train(tokenized_new_functions, total_examples=len(tokenized_new_functions), epochs=5)
    
    existing_model.save(MODEL_PATH)
    print("‚úÖ –ú–æ–¥–µ–ª—ä—Ç –±–µ—à–µ —É—Å–ø–µ—à–Ω–æ –¥–æ–æ–±—É—á–µ–Ω –∏ –∑–∞–ø–∞–∑–µ–Ω.")
    
elif not existing_model:
    print("üÜï –°—ä–∑–¥–∞–≤–∞–º–µ –Ω–æ–≤ –º–æ–¥–µ–ª –æ—Ç –Ω—É–ª–∞—Ç–∞...")
    tokenized_all_functions = [tokenize_function_code(func) for func in new_functions]
    new_model = Word2Vec(tokenized_all_functions, vector_size=100, window=5, min_count=1, workers=4)
    new_model.save(MODEL_PATH)
    print("‚úÖ –ù–æ–≤–∏—è—Ç –º–æ–¥–µ–ª –µ –æ–±—É—á–µ–Ω –∏ –∑–∞–ø–∞–∑–µ–Ω.")

## pip install watchdog ##
## –°–∫—Ä–∏–ø—Ç –∑–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–æ–º–µ–Ω–∏ ##
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import os

class CodeChangeHandler(FileSystemEventHandler):
    def on_modified(self, event):
        if event.src_path.endswith(".py"):  # –°–∞–º–æ Python —Ñ–∞–π–ª–æ–≤–µ
            print(f"üîÑ –ö–æ–¥—ä—Ç —Å–µ –ø—Ä–æ–º–µ–Ω–∏: {event.src_path}")
            retrain_model()

def watch_directory(directory):
    event_handler = CodeChangeHandler()
    observer = Observer()
    observer.schedule(event_handler, directory, recursive=True)
    observer.start()
    print(f"üëÄ –°–ª–µ–¥–µ–Ω–µ –Ω–∞ {directory} –∑–∞ –ø—Ä–æ–º–µ–Ω–∏...")

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()

def retrain_model():
    """ –ü—Ä–µ–æ–±—É—á–∞–≤–∞ –º–æ–¥–µ–ª–∞ —Å–∞–º–æ –∞–∫–æ –∏–º–∞ –Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏. """
    existing_model = load_existing_model()
    new_functions = get_new_functions(existing_model.wv.index_to_key if existing_model else [], "my_project/")

    if new_functions:
        print("üîÑ –î–æ–æ–±—É—á–∞–≤–∞–º–µ –º–æ–¥–µ–ª–∞ —Å –Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏...")
        tokenized_new_functions = [tokenize_function_code(func) for func in new_functions]
        
        if existing_model:
            existing_model.build_vocab(tokenized_new_functions, update=True)
            existing_model.train(tokenized_new_functions, total_examples=len(tokenized_new_functions), epochs=5)
            existing_model.save(MODEL_PATH)
        else:
            new_model = Word2Vec(tokenized_new_functions, vector_size=100, window=5, min_count=1, workers=4)
            new_model.save(MODEL_PATH)

        print("‚úÖ –ú–æ–¥–µ–ª—ä—Ç –µ –æ–±–Ω–æ–≤–µ–Ω.")

# –°—Ç–∞—Ä—Ç–∏—Ä–∞–º–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ—Ç–æ –Ω–∞ –ø–∞–ø–∫–∞—Ç–∞
watch_directory("my_project/")

## –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –ª–æ–≥ —Ñ–∞–π–ª ##
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ –ª–æ–≥ —Ñ–∞–π–ª–∞
logging.basicConfig(
    filename="training_log.txt",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

def log_training_event(event_message):
    """ –ó–∞–ø–∏—Å–≤–∞ —Å—ä–±–∏—Ç–∏–µ –≤ training_log.txt """
    logging.info(event_message)
    print(event_message)  # –ü–æ–∫–∞–∑–≤–∞ —Å—ä–æ–±—â–µ–Ω–∏–µ—Ç–æ –∏ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞

## –ê–∫—Ç—É–∞–ª–∏–∑–∏—Ä–∞–Ω–µ –Ω–∞ retrain_model() –¥–∞ –∑–∞–ø–∏—Å–≤–∞ –ª–æ–≥–æ–≤–µ ##
def retrain_model():
    """ –ü—Ä–µ–æ–±—É—á–∞–≤–∞ –º–æ–¥–µ–ª–∞ —Å–∞–º–æ –∞–∫–æ –∏–º–∞ –Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∑–∞–ø–∏—Å–≤–∞ –ª–æ–≥–æ–≤–µ. """
    existing_model = load_existing_model()
    new_functions = get_new_functions(existing_model.wv.index_to_key if existing_model else [], "my_project/")

    if new_functions:
        log_training_event(f"üîÑ –û—Ç–∫—Ä–∏—Ç–∏ {len(new_functions)} –Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞ –æ–±—É—á–µ–Ω–∏–µ.")

        tokenized_new_functions = [tokenize_function_code(func) for func in new_functions]
        
        if existing_model:
            existing_model.build_vocab(tokenized_new_functions, update=True)
            existing_model.train(tokenized_new_functions, total_examples=len(tokenized_new_functions), epochs=5)
            existing_model.save(MODEL_PATH)
        else:
            new_model = Word2Vec(tokenized_new_functions, vector_size=100, window=5, min_count=1, workers=4)
            new_model.save(MODEL_PATH)

        log_training_event("‚úÖ –ú–æ–¥–µ–ª—ä—Ç –µ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–µ–Ω.")
    else:
        log_training_event("‚úÖ –ù—è–º–∞ –Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏. –û–±—É—á–µ–Ω–∏–µ—Ç–æ –Ω–µ –µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ.")

## –î–æ–±–∞–≤—è–Ω–µ –Ω–∞ –ª–æ–≥–æ–≤–µ –ø—Ä–∏ —Å–ª–µ–¥–µ–Ω–µ –Ω–∞ —Ñ–∞–π–ª–æ–≤–µ—Ç–µ ##
class CodeChangeHandler(FileSystemEventHandler):
    def on_modified(self, event):
        if event.src_path.endswith(".py"):
            log_training_event(f"üìÇ –§–∞–π–ª—ä—Ç {event.src_path} –±–µ—à–µ –ø—Ä–æ–º–µ–Ω–µ–Ω.")
            retrain_model()

## –ò–∑–ø—Ä–∞—â–∞–Ω–µ –Ω–∞ –ª–æ–≥–æ–≤–µ—Ç–µ –ø–æ Email (SMTP) ##
import smtplib
import os
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

EMAIL_ADDRESS = "your_email@gmail.com"
EMAIL_PASSWORD = "your_app_password"  # 16-—Ü–∏—Ñ—Ä–µ–Ω App Password

def send_email():
    """ –ò–∑–ø—Ä–∞—â–∞ training_log.txt –∫–∞—Ç–æ email. """
    if not os.path.exists("training_log.txt"):
        print("‚ö†Ô∏è –ù—è–º–∞ training_log.txt, –Ω—è–º–∞ –∫–∞–∫–≤–æ –¥–∞ –∏–∑–ø—Ä–∞—Ç—è.")
        return

    with open("training_log.txt", "r", encoding="utf-8") as file:
        log_content = file.read()

    msg = MIMEMultipart()
    msg["From"] = EMAIL_ADDRESS
    msg["To"] = EMAIL_ADDRESS  # –ú–æ–∂–µ –¥–∞ –¥–æ–±–∞–≤–∏—à –¥—Ä—É–≥ –ø–æ–ª—É—á–∞—Ç–µ–ª
    msg["Subject"] = "üîÑ AI Training Log Update"
    
    msg.attach(MIMEText(log_content, "plain"))

    try:
        server = smtplib.SMTP("smtp.gmail.com", 587)
        server.starttls()
        server.login(EMAIL_ADDRESS, EMAIL_PASSWORD)
        server.sendmail(EMAIL_ADDRESS, EMAIL_ADDRESS, msg.as_string())
        server.quit()
        print("‚úÖ –õ–æ–≥–æ–≤–µ—Ç–µ –±—è—Ö–∞ –∏–∑–ø—Ä–∞—Ç–µ–Ω–∏ –ø–æ email.")
    except Exception as e:
        print("‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–ø—Ä–∞—â–∞–Ω–µ –Ω–∞ email:", str(e))

# –ò–∑–ø—Ä–∞—â–∞–º–µ email —Å–ª–µ–¥ –æ–±—É—á–µ–Ω–∏–µ
retrain_model()
send_email()

## –ò–∑–ø—Ä–∞—â–∞–Ω–µ –Ω–∞ –ª–æ–≥–æ–≤–µ—Ç–µ –≤ Telegram ##
# –ê–∫–æ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—à Telegram, —â–µ –∏–∑–ø–æ–ª–∑–≤–∞–º–µ Telegram Bot API.
# 2.1 –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ Telegram –±–æ—Ç
# –û—Ç–∏–¥–∏ –≤ Telegram –∏ –ø–æ—Ç—ä—Ä—Å–∏ @BotFather
# –ù–∞–ø–∏—à–∏:
# /newbot
# –î–∞–π –∏–º–µ –∏ username –Ω–∞ –±–æ—Ç–∞
# –ü–æ–ª—É—á–∞–≤–∞—à API –∫–ª—é—á (–∑–∞–ø–∞–∑–∏ –≥–æ)
# 2.2 –ü–æ–ª—É—á–∞–≤–∞–Ω–µ –Ω–∞ Telegram Chat ID
# –û—Ç–∏–¥–∏ –Ω–∞ https://t.me/userinfobot
# –ù–∞–ø–∏—à–∏ /start
# –©–µ –ø–æ–ª—É—á–∏—à —Ç–≤–æ—è Chat ID

import requests

TELEGRAM_BOT_TOKEN = "your_telegram_bot_token"
TELEGRAM_CHAT_ID = "your_chat_id"

def send_telegram_message():
    """ –ò–∑–ø—Ä–∞—â–∞ training_log.txt –∫–∞—Ç–æ Telegram —Å—ä–æ–±—â–µ–Ω–∏–µ. """
    if not os.path.exists("training_log.txt"):
        print("‚ö†Ô∏è –ù—è–º–∞ training_log.txt, –Ω—è–º–∞ –∫–∞–∫–≤–æ –¥–∞ –∏–∑–ø—Ä–∞—Ç—è.")
        return

    with open("training_log.txt", "r", encoding="utf-8") as file:
        log_content = file.read()

    message = f"üîÑ AI Training Log Update:\n\n{log_content[-4000:]}"  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ—Ç 4096 —Å–∏–º–≤–æ–ª–∞

    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    params = {"chat_id": TELEGRAM_CHAT_ID, "text": message}

    response = requests.post(url, params=params)
    if response.status_code == 200:
        print("‚úÖ –õ–æ–≥–æ–≤–µ—Ç–µ –±—è—Ö–∞ –∏–∑–ø—Ä–∞—Ç–µ–Ω–∏ –≤ Telegram.")
    else:
        print("‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–ø—Ä–∞—â–∞–Ω–µ –≤ Telegram:", response.text)

# –ò–∑–ø—Ä–∞—â–∞–º–µ –≤ Telegram —Å–ª–µ–¥ –æ–±—É—á–µ–Ω–∏–µ
retrain_model()
send_telegram_message()

## –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ systemd —É—Å–ª—É–≥–∞ (service) ##
sudo vim /etc/systemd/system/code_training.service
[Unit]
Description=AI Code Training Service
After=network.target

[Service]
ExecStart=/usr/bin/python3 /path/to/your/script.py
WorkingDirectory=/path/to/your/project
User=your_username
Group=your_group
Restart=always
RestartSec=10
StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=code_training

[Install]
WantedBy=multi-user.target

sudo systemctl daemon-reload
sudo systemctl enable code_training.service
sudo systemctl start code_training.service
sudo systemctl status code_training.service
journalctl -u code_training.service -f


## –ò–∑–ø—Ä–∞—â–∞–Ω–µ –Ω–∞ –∏–∑–≤–µ—Å—Ç–∏—è –ø—Ä–∏ –≥—Ä–µ—à–∫–∞ ##
import smtplib
from email.mime.text import MIMEText

def send_error_notification(error_message):
    """ –ò–∑–ø—Ä–∞—â–∞ –∏–∑–≤–µ—Å—Ç–∏–µ –∑–∞ –≥—Ä–µ—à–∫–∞ –ø–æ email """
    try:
        # –ò–∑–ø–æ–ª–∑–≤–∞—à —Å—ä—â–∏—è –∫–æ–¥ –∑–∞ –∏–∑–ø—Ä–∞—â–∞–Ω–µ –Ω–∞ email, –∫–∞–∫—Ç–æ –ø–æ-–≥–æ—Ä–µ
        msg = MIMEText(f"‚ùå –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–ø—ä–ª–Ω–µ–Ω–∏–µ –Ω–∞ —Å–∫—Ä–∏–ø—Ç–∞:\n\n{error_message}")
        msg["From"] = "your_email@gmail.com"
        msg["To"] = "your_email@gmail.com"
        msg["Subject"] = "AI Training Script Error"
        
        server = smtplib.SMTP("smtp.gmail.com", 587)
        server.starttls()
        server.login("your_email@gmail.com", "your_app_password")
        server.sendmail("your_email@gmail.com", "your_email@gmail.com", msg.as_string())
        server.quit()
        print("‚úÖ –ò–∑–≤–µ—Å—Ç–∏–µ –∑–∞ –≥—Ä–µ—à–∫–∞ –∏–∑–ø—Ä–∞—Ç–µ–Ω–æ.")
    except Exception as e:
        print(f"‚ùå –ù–µ—É—Å–ø–µ—à–Ω–æ –∏–∑–ø—Ä–∞—â–∞–Ω–µ –Ω–∞ –∏–∑–≤–µ—Å—Ç–∏–µ –∑–∞ –≥—Ä–µ—à–∫–∞: {str(e)}")

# –ü—Ä–∏–º–µ—Ä –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –≥—Ä–µ—à–∫–∞
try:
    retrain_model()
except Exception as e:
    send_error_notification(str(e))

## –ò–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ Docker –∑–∞ –∏–∑–æ–ª–∏—Ä–∞–Ω–∞ —Å—Ä–µ–¥–∞ ##
FROM python:3.9-slim

WORKDIR /app
COPY . /app

RUN pip install --no-cache-dir -r requirements.txt

CMD ["python3", "script.py"]

## –ü–æ–¥–æ–±—Ä—è–≤–∞–Ω–µ –Ω–∞ –ª–æ–≥–æ–≤–µ—Ç–µ —Å –ø–æ–≤–µ—á–µ –¥–µ—Ç–∞–π–ª–∏ ##
import time

def log_training_event(event_message):
    """ –ó–∞–ø–∏—Å–≤–∞ —Å—ä–±–∏—Ç–∏–µ –≤ training_log.txt —Å –¥–æ–ø—ä–ª–Ω–∏—Ç–µ–ª–Ω–∏ –¥–µ—Ç–∞–π–ª–∏. """
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
    log_message = f"{timestamp} - {event_message}"
    logging.info(log_message)
    print(log_message)  # –ü–æ–∫–∞–∑–≤–∞ —Å—ä–æ–±—â–µ–Ω–∏–µ—Ç–æ –∏ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞

## –ü–ª–∞–Ω–∏—Ä–∞–Ω–µ –Ω–∞ —Ä–µ–¥–æ–≤–Ω–æ –æ–±—É—á–µ–Ω–∏–µ (Cron Jobs) ##
crontab -e
0 3 * * * /usr/bin/python3 /path/to/your/script.py

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∞ (–∑–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞ —Ä–µ—Å—É—Ä—Å–∏—Ç–µ) ##
import psutil

def monitor_system_resources():
    """ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ä–∞ —Å–∏—Å—Ç–µ–º–Ω–∏—Ç–µ —Ä–µ—Å—É—Ä—Å–∏ –ø–æ –≤—Ä–µ–º–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ—Ç–æ """
    cpu_usage = psutil.cpu_percent(interval=1)
    memory_info = psutil.virtual_memory()
    
    print(f"CPU Usage: {cpu_usage}%")
    print(f"Memory Usage: {memory_info.percent}%")

# –ü—Ä–∏–º–µ—Ä –∑–∞ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ:
monitor_system_resources()

## –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ##
def load_model(model_path):
    """ –ó–∞—Ä–µ–∂–¥–∞ —Å—ä—â–µ—Å—Ç–≤—É–≤–∞—â–∏—è –º–æ–¥–µ–ª –æ—Ç –ø—ä—Ç—è. """
    try:
        model = Word2Vec.load(model_path)
        print("–ú–æ–¥–µ–ª—ä—Ç –±–µ—à–µ —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–¥–µ–Ω.")
        return model
    except Exception as e:
        print(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞: {e}")
        return None

def retrain_model(model, new_functions, epochs=5, vector_size=100):
    """ –û–±—É—á–∞–≤–∞ –º–æ–¥–µ–ª–∞ —Å –Ω–æ–≤–∏ —Ñ—É–Ω–∫—Ü–∏–∏. """
    tokenized_new_functions = [tokenize_function_code(func) for func in new_functions]
    
    if model:
        model.build_vocab(tokenized_new_functions, update=True)
        model.train(tokenized_new_functions, total_examples=len(tokenized_new_functions), epochs=epochs)
        model.save("model.bin")
        print("–ú–æ–¥–µ–ª—ä—Ç –±–µ—à–µ –æ–±–Ω–æ–≤–µ–Ω —É—Å–ø–µ—à–Ω–æ.")
    else:
        print("–ù—è–º–∞ –∑–∞—Ä–µ–¥–µ–Ω –º–æ–¥–µ–ª –∑–∞ –æ–±—É—á–µ–Ω–∏–µ.")

def send_email_notification(subject, body, to_email="your_email@gmail.com"):
    """ –ò–∑–ø—Ä–∞—â–∞ email –∏–∑–≤–µ—Å—Ç–∏–µ –ø—Ä–∏ –≥—Ä–µ—à–∫–∞ –∏–ª–∏ —Å—ä–±–∏—Ç–∏–µ. """
    try:
        msg = MIMEText(body)
        msg["From"] = "your_email@gmail.com"
        msg["To"] = to_email
        msg["Subject"] = subject
        
        server = smtplib.SMTP("smtp.gmail.com", 587)
        server.starttls()
        server.login("your_email@gmail.com", "your_app_password")
        server.sendmail("your_email@gmail.com", to_email, msg.as_string())
        server.quit()
        print("–ò–∑–≤–µ—Å—Ç–∏–µ –∑–∞ –≥—Ä–µ—à–∫–∞ –µ –∏–∑–ø—Ä–∞—Ç–µ–Ω–æ.")
    except Exception as e:
        print(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∏–∑–ø—Ä–∞—â–∞–Ω–µ –Ω–∞ email: {str(e)}")


def tokenize_function_code(function_code):
    """
    –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–∞ Python –∫–æ–¥.
    
    :param function_code: –°—Ç—Ä–∏–Ω–≥ —Å Python –∫–æ–¥
    :return: –õ–∏—Å—Ç —Å —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–∞–Ω–∏ –¥—É–º–∏
    """
    # –¢—É–∫ –∏–∑–ø–æ–ª–∑–≤–∞–º–µ –ø—Ä–∏–º–µ—Ä–µ–Ω –º–µ—Ö–∞–Ω–∏–∑—ä–º –∑–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = function_code.split()
    return tokens


class ModelTrainer:
    def __init__(self, model_path):
        self.model_path = model_path
        self.model = self.load_model()

    def load_model(self):
        """ –ó–∞—Ä–µ–∂–¥–∞ –º–æ–¥–µ–ª. """
        try:
            model = Word2Vec.load(self.model_path)
            print("–ú–æ–¥–µ–ª—ä—Ç –±–µ—à–µ —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–µ–¥–µ–Ω.")
            return model
        except Exception as e:
            print(f"–ì—Ä–µ—à–∫–∞ –ø—Ä–∏ –∑–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞: {e}")
            return None

    def retrain_model(self, new_functions):
        """ –ü—Ä–µ–æ–±—É—á–∞–≤–∞ –º–æ–¥–µ–ª–∞. """
        if self.model:
            tokenized_new_functions = [tokenize_function_code(func) for func in new_functions]
            self.model.build_vocab(tokenized_new_functions, update=True)
            self.model.train(tokenized_new_functions, total_examples=len(tokenized_new_functions), epochs=5)
            self.model.save(self.model_path)
            print("–ú–æ–¥–µ–ª—ä—Ç –±–µ—à–µ –æ–±–Ω–æ–≤–µ–Ω —É—Å–ø–µ—à–Ω–æ.")
        else:
            print("–ù—è–º–∞ –∑–∞—Ä–µ–¥–µ–Ω –º–æ–¥–µ–ª –∑–∞ –æ–±—É—á–µ–Ω–∏–µ.")


import logging

logging.basicConfig(filename="training_log.txt", level=logging.INFO)

def log_training_event(event_message):
    """ –ó–∞–ø–∏—Å–≤–∞ —Å—ä–±–∏—Ç–∏–µ –≤ training_log.txt """
    logging.info(event_message)
    print(event_message)  # –ü–æ–∫–∞–∑–≤–∞ —Å—ä–æ–±—â–µ–Ω–∏–µ—Ç–æ –∏ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞
import logging

logging.basicConfig(filename="training_log.txt", level=logging.INFO)

def log_training_event(event_message):
    """ –ó–∞–ø–∏—Å–≤–∞ —Å—ä–±–∏—Ç–∏–µ –≤ training_log.txt """
    logging.info(event_message)
    print(event_message)  # –ü–æ–∫–∞–∑–≤–∞ —Å—ä–æ–±—â–µ–Ω–∏–µ—Ç–æ –∏ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞


## unit test ##
def test_tokenize_function_code():
    result = tokenize_function_code("def hello_world(): print('Hello World')")
    assert result == ['def', 'hello_world()', ':', 'print(', "'Hello", 'World'", ')']


### pipe / –∫–æ–Ω–≤–µ–π–Ω–µ—Ä ###
def read_data(file_path):
    """–ß–µ—Ç–µ —Å—ä–¥—ä—Ä–∂–∞–Ω–∏–µ –Ω–∞ —Ñ–∞–π–ª –∏ –≤—Ä—ä—â–∞ —Ç–µ–∫—Å—Ç–∞ –∫–∞—Ç–æ –Ω–∏–∑."""
    with open(file_path, 'r') as f:
        data = f.read()
    return data

def tokenize_data(data):
    """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä–∞ —Ç–µ–∫—Å—Ç–∞ (—Ä–∞–∑–¥–µ–ª—è –Ω–∞ –æ—Ç–¥–µ–ª–Ω–∏ –¥—É–º–∏)."""
    return data.split()

def filter_stopwords(tokens, stopwords):
    """–§–∏–ª—Ç—Ä–∏—Ä–∞ —á–µ—Å—Ç–æ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–∏ –¥—É–º–∏ (–∫–∞—Ç–æ "–∏", "–Ω–∞" –∏ —Ç.–Ω.)."""
    return [token for token in tokens if token not in stopwords]

def count_tokens(tokens):
    """–ü—Ä–µ–±—Ä–æ—è–≤–∞ –∫–æ–ª–∫–æ –ø—ä—Ç–∏ —Å–µ —Å—Ä–µ—â–∞ –≤—Å—è–∫–∞ –¥—É–º–∞."""
    return {token: tokens.count(token) for token in set(tokens)}

def display_results(results):
    """–ü–æ–∫–∞–∑–≤–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ (–±—Ä–æ—è –Ω–∞ –≤—Å—è–∫–∞ –¥—É–º–∞)."""
    for word, count in results.items():
        print(f"{word}: {count}")

# –ü–æ–¥–≥–æ—Ç–≤—è–º–µ —Å–ø–∏—Ä–∞–ª–∞—Ç–∞ (pipe) —Å –¥–∞–Ω–Ω–∏
stopwords = {"–∏", "–Ω–∞", "–æ—Ç", "–∑–∞", "—Å", "–ø–æ", "–∫–∞—Ç–æ"}

file_path = "text.txt"
result = (read_data(file_path)  # –ß–µ—Ç–µ–º –¥–∞–Ω–Ω–∏ –æ—Ç —Ñ–∞–π–ª
          | tokenize_data  # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–∞–º–µ –¥–∞–Ω–Ω–∏—Ç–µ
          | filter_stopwords(stopwords)  # –§–∏–ª—Ç—Ä–∏—Ä–∞–º–µ —Å—Ç–æ–ø-–¥—É–º–∏
          | count_tokens  # –ü—Ä–µ–±—Ä–æ—è–≤–∞–º–µ —á–µ—Å—Ç–æ—Ç–∞—Ç–∞ –Ω–∞ –¥—É–º–∏—Ç–µ
          | display_results)  # –ü–æ–∫–∞–∑–≤–∞–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ

## custom pipe ##
class Pipe:
    def __init__(self, func):
        self.func = func

    def __or__(self, other):
        """ –ü–æ–∑–≤–æ–ª—è–≤–∞ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ pipe (|) –∑–∞ —Å–≤—ä—Ä–∑–≤–∞–Ω–µ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏. """
        return Pipe(lambda x: other.func(self.func(x)))

    def __call__(self, data):
        """ –ò–∑–≤–∏–∫–≤–∞ —Ñ—É–Ω–∫—Ü–∏—è—Ç–∞ –Ω–∞ pipe —Å –≤—Ö–æ–¥–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏. """
        return self.func(data)

# –ü—Ä–∏–º–µ—Ä –∑–∞ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ –Ω–∞ pipe –≤ Python
read = Pipe(read_data)
tokenize = Pipe(tokenize_data)
filter = Pipe(lambda data: filter_stopwords(data, stopwords))
count = Pipe(count_tokens)
display = Pipe(display_results)

result = (read("text.txt")
          | tokenize
          | filter
          | count
          | display)

## –ø—Ä–∏–º–µ—Ä ##
def load_data(file_path):
    """–ß–µ—Ç–µ –¥–∞–Ω–Ω–∏ –æ—Ç —Ñ–∞–π–ª –∏ –≥–∏ –≤—Ä—ä—â–∞ –∫–∞—Ç–æ —Å–ø–∏—Å—ä–∫."""
    with open(file_path, "r") as f:
        return f.readlines()

def preprocess_data(data):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–≤–∞ –¥–∞–Ω–Ω–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä –ø—Ä–µ–º–∞—Ö–≤–∞ —Å–∏–º–≤–æ–ª–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–∞ –∏ —Ç.–Ω.)."""
    return [d.strip().lower() for d in data]

def train_model(data):
    """–û–±—É—á–∞–≤–∞ –º–æ–¥–µ–ª —Å –¥–∞–Ω–Ω–∏."""
    model = SomeMachineLearningModel()
    model.train(data)
    return model

def evaluate_model(model, test_data):
    """–û—Ü–µ–Ω—è–≤–∞ –º–æ–¥–µ–ª–∞ —Å —Ç–µ—Å—Ç–æ–≤–∏ –¥–∞–Ω–Ω–∏."""
    accuracy = model.evaluate(test_data)
    return accuracy

def display_results(accuracy):
    """–ü–æ–∫–∞–∑–≤–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ –æ—Ç –æ—Ü–µ–Ω–∫–∞—Ç–∞."""
    print(f"–ú–æ–¥–µ–ª—ä—Ç –∏–º–∞ —Ç–æ—á–Ω–æ—Å—Ç: {accuracy}%")

pipeline = (Pipe(load_data)
            | Pipe(preprocess_data)
            | Pipe(train_model)
            | Pipe(lambda model: evaluate_model(model, test_data))
            | Pipe(display_results))

# –°—Ç–∞—Ä—Ç–∏—Ä–∞—à –∫–æ–Ω–≤–µ–π–µ—Ä–∞ —Å —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω–∏
pipeline("data.txt")

{
  "data": {
    "text": "–¢–æ–≤–∞ –µ –ø—Ä–∏–º–µ—Ä–µ–Ω —Ç–µ–∫—Å—Ç –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞.",
    "numbers": [1, 2, 3, 4, 5],
    "timestamp": "2025-03-30"
  },
  "metadata": {
    "language": "bg",
    "operation": "tokenization"
  }
}

import json

class Pipe:
    def __init__(self, func):
        self.func = func

    def __or__(self, other):
        """–ü–æ–∑–≤–æ–ª—è–≤–∞ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ—Ç–æ –Ω–∞ pipe (|) –∑–∞ —Å–≤—ä—Ä–∑–≤–∞–Ω–µ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏."""
        return Pipe(lambda data: other.func(self.func(data)))

    def __call__(self, data):
        """–ò–∑–≤–∏–∫–≤–∞ —Ñ—É–Ω–∫—Ü–∏—è—Ç–∞ –Ω–∞ pipe —Å –≤—Ö–æ–¥–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏."""
        return self.func(data)

def process_text(data):
    """–ü—Ä–∏–º–µ—Ä–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ —Ç–µ–∫—Å—Ç –æ—Ç JSON."""
    text = data.get("data", {}).get("text", "")
    if text:
        processed_text = text.lower()
        data["data"]["text"] = processed_text
    return data

def extract_numbers(data):
    """–ü—Ä–∏–º–µ—Ä–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞ –∏–∑–≤–ª–∏—á–∞–Ω–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ —á–∏—Å–ª–æ–≤–∏ –¥–∞–Ω–Ω–∏ –æ—Ç JSON."""
    numbers = data.get("data", {}).get("numbers", [])
    squared_numbers = [n ** 2 for n in numbers]
    data["data"]["numbers"] = squared_numbers
    return data

def filter_metadata(data):
    """–ü—Ä–∏–º–µ—Ä–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –∑–∞ —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ –Ω–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω–∏ –æ—Ç JSON."""
    metadata = data.get("metadata", {})
    if metadata.get("operation") == "tokenization":
        data["metadata"]["status"] = "Processing"
    return data

def display_results(data):
    """–§—É–Ω–∫—Ü–∏—è –∑–∞ –ø–æ–∫–∞–∑–≤–∞–Ω–µ –Ω–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ –æ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∞—Ç–∞."""
    print("–û–±—Ä–∞–±–æ—Ç–µ–Ω–∏ –¥–∞–Ω–Ω–∏:")
    print(json.dumps(data, indent=2))
    return data

# –ü—Ä–∏–º–µ—Ä–µ–Ω JSON, —Å –∫–æ–π—Ç–æ —Ä–∞–±–æ—Ç–∏–º
data = {
    "data": {
        "text": "–¢–æ–≤–∞ –µ –ø—Ä–∏–º–µ—Ä–µ–Ω —Ç–µ–∫—Å—Ç –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞.",
        "numbers": [1, 2, 3, 4, 5],
        "timestamp": "2025-03-30"
    },
    "metadata": {
        "language": "bg",
        "operation": "tokenization"
    }
}

# –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ pipe —Å —Ñ—É–Ω–∫—Ü–∏–∏
result = (Pipe(lambda data: data)  # –ó–∞–ø–æ—á–≤–∞–º–µ —Å –≤—Ö–æ–¥–Ω–∏—è JSON
          | process_text           # –û–±—Ä–∞–±–æ—Ç–≤–∞–º–µ —Ç–µ–∫—Å—Ç–∞
          | extract_numbers        # –û–±—Ä–∞–±–æ—Ç–≤–∞–º–µ —á–∏—Å–ª–∞—Ç–∞
          | filter_metadata        # –§–∏–ª—Ç—Ä–∏—Ä–∞–º–µ –º–µ—Ç–∞–¥–∞–Ω–Ω–∏—Ç–µ
          | display_results)       # –ü–æ–∫–∞–∑–≤–∞–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ

# –ò–∑–ø—ä–ª–Ω—è–≤–∞–º–µ –∫–æ–Ω–≤–µ–π–µ—Ä–∞
result(data)

def filter_metadata(data):
    """–§–∏–ª—Ç—Ä–∏—Ä–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω–∏ –∏ –ø—Ä–∏ –Ω—É–∂–¥–∞ –∏–∑–≤—ä—Ä—à–≤–∞ —Ä–∞–∑–ª–∏—á–Ω–∏ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç –æ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω–∏—Ç–µ."""
    metadata = data.get("metadata", {})
    
    if metadata.get("operation") == "tokenization":
        data["metadata"]["status"] = "Tokenizing text"
    elif metadata.get("operation") == "summarization":
        data["metadata"]["status"] = "Summarizing text"
    else:
        data["metadata"]["status"] = "Unknown operation"
    
    return data

class Pipe:
    def __init__(self, func):
        self.func = func

    def __or__(self, other):
        """ –ü–æ–∑–≤–æ–ª—è–≤–∞ –∏–∑–ø–æ–ª–∑–≤–∞–Ω–µ—Ç–æ –Ω–∞ pipe (|) –∑–∞ —Å–≤—ä—Ä–∑–≤–∞–Ω–µ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏. """
        return Pipe(lambda data: other.func(self.func(data)))

    def __call__(self, data):
        """ –ò–∑–≤–∏–∫–≤–∞ —Ñ—É–Ω–∫—Ü–∏—è—Ç–∞ –Ω–∞ pipe —Å –≤—Ö–æ–¥–Ω–∏—Ç–µ –¥–∞–Ω–Ω–∏. """
        return self.func(data)

def process_text(data):
    """–û–±—Ä–∞–±–æ—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç JSON."""
    text = data.get("data", {}).get("text", "")
    if text:
        processed_text = text.lower()
        data["data"]["text"] = processed_text
    return data

def filter_metadata(data):
    """–§–∏–ª—Ç—Ä–∏—Ä–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–≤–∞ –æ–ø–µ—Ä–∞—Ü–∏—è—Ç–∞ —Å–ø–æ—Ä–µ–¥ —Ç–µ–∫—É—â–æ—Ç–æ —Å—ä—Å—Ç–æ—è–Ω–∏–µ."""
    metadata = data.get("metadata", {})
    if metadata.get("operation") == "tokenization":
        data["metadata"]["status"] = "Tokenizing text"
    elif metadata.get("operation") == "summarization":
        data["metadata"]["status"] = "Summarizing text"
    else:
        data["metadata"]["status"] = "Unknown operation"
    return data

# –ü—Ä–∏–º–µ—Ä–µ–Ω JSON
data = {
    "data": {
        "text": "–¢–æ–≤–∞ –µ –ø—Ä–∏–º–µ—Ä–µ–Ω —Ç–µ–∫—Å—Ç –∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞.",
        "numbers": [1, 2, 3, 4, 5],
        "timestamp": "2025-03-30"
    },
    "metadata": {
        "language": "bg",
        "operation": "tokenization"
    }
}

# –°—ä–∑–¥–∞–≤–∞–Ω–µ –Ω–∞ pipeline
result = (Pipe(lambda data: data)  # –ó–∞–ø–æ—á–≤–∞–º–µ —Å –≤—Ö–æ–¥–Ω–∏—è JSON
          | process_text           # –û–±—Ä–∞–±–æ—Ç–≤–∞–º–µ —Ç–µ–∫—Å—Ç–∞
          | extract_numbers        # –û–±—Ä–∞–±–æ—Ç–≤–∞–º–µ —á–∏—Å–ª–∞—Ç–∞
          | filter_metadata        # –§–∏–ª—Ç—Ä–∏—Ä–∞–º–µ –º–µ—Ç–∞–¥–∞–Ω–Ω–∏—Ç–µ
          | display_results)       # –ü–æ–∫–∞–∑–≤–∞–º–µ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏—Ç–µ

# –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ pipeline
result(data)
